{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48b876be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-14 11:49:30.764435: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64\n",
      "2022-01-14 11:49:30.764482: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "except:\n",
    "    !pip install transformers\n",
    "    from transformers import  AutoTokenizer,AutoModel\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bab17ab-dac5-4c9e-92fb-0695b78f5b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Davlan/bert-base-multilingual-cased-ner-hrl were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at Davlan/bert-base-multilingual-cased-ner-hrl and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('Davlan/bert-base-multilingual-cased-ner-hrl')\n",
    "model = AutoModel.from_pretrained('Davlan/bert-base-multilingual-cased-ner-hrl',output_attentions=True).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "138755ce-9dc7-42d0-b32a-916fcae3f83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodings_context(sentence, keywords, tokenizer, model):\n",
    "    tokens =  tokenizer.encode(sentence,max_length=512, truncation=True)\n",
    "\n",
    "    extracted_keywords = []\n",
    "    indices = []\n",
    "    for keyword in keywords:\n",
    "        keyword_tokens = tokenizer.encode(keyword)[1:-1]\n",
    "        index = -1\n",
    "        for i in range(len(tokens)):\n",
    "            if tokens[i:i+len(keyword_tokens)] == keyword_tokens:\n",
    "                index=i\n",
    "                break\n",
    "        if index >=0:\n",
    "            indices.append((i,i+len(keyword_tokens)))\n",
    "            extracted_keywords.append(keyword)\n",
    "    out = model(torch.tensor([tokens]).cuda())\n",
    "    embeddings = []\n",
    "    for s, e in indices:     \n",
    "        embeddings.append(out.last_hidden_state[0][s:e].mean(0).detach().cpu().numpy())\n",
    "    return extracted_keywords, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9104d46-1e1b-4c89-9580-1f77d7f5f54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_context(sentence, keyword, tokenizer, model):\n",
    "    tokens = tokenizer.encode(sentence)\n",
    "    keyword_tokens = tokenizer.encode(keyword)[1:-1]\n",
    "    index = -1\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i:i+len(keyword_tokens)] == keyword_tokens:\n",
    "            index=i\n",
    "            break\n",
    "    assert index >=0, \"Keyword '{}' not found in '{}'\".format(keyword, sentence)\n",
    "    out = model(torch.tensor([tokens]).cuda())\n",
    "    return out.last_hidden_state[0][i:i+len(keyword_tokens)].mean(0).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "269b3268-38dc-4135-aa94-6402202284b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1777/1777 [00:32<00:00, 55.37it/s]\n",
      "100%|███████████████████████████████████████| 1777/1777 [00:31<00:00, 55.78it/s]\n",
      "100%|███████████████████████████████████████| 2164/2164 [00:39<00:00, 55.23it/s]\n",
      "100%|██████████████████████████████████████| 2164/2164 [00:04<00:00, 497.88it/s]\n",
      "100%|███████████████████████████████████████| 1040/1040 [02:36<00:00,  6.63it/s]\n",
      "100%|█████████████████████████████████████████| 229/229 [08:15<00:00,  2.17s/it]\n"
     ]
    }
   ],
   "source": [
    "tech_embeddings_de = []\n",
    "df = pd.read_csv('./domain_sentences/c4istar_technologies_sentences_de.csv')\n",
    "for i in tqdm(range(len(df))):\n",
    "    try:\n",
    "        embedding = encoding_context(df['sentence'].iloc[i].lower(), df['kw'].iloc[i].lower(), tokenizer, model)\n",
    "    except:\n",
    "        pass\n",
    "        # print(df['sentence'].iloc[i], [df['kw'].iloc[i]])\n",
    "    else:\n",
    "        tech_embeddings_de.append(embedding)\n",
    "tech_embeddings_fr = []\n",
    "df = pd.read_csv('./domain_sentences/c4istar_technologies_sentences_fr.csv')\n",
    "for i in tqdm(range(len(df))):\n",
    "    try:\n",
    "        embedding = encoding_context(df['sentence'].iloc[i].lower(), df['kw'].iloc[i].lower(), tokenizer, model)\n",
    "    except:\n",
    "        pass\n",
    "        # print(df['sentence'].iloc[i], [df['kw'].iloc[i]])\n",
    "    else:\n",
    "        tech_embeddings_fr.append(embedding)\n",
    "eco_embeddings_de = []\n",
    "df = pd.read_csv('./domain_sentences/eco_sentences_de.csv')\n",
    "for i in tqdm(range(len(df))):\n",
    "    try:\n",
    "        embedding = encoding_context(df['sentence'].iloc[i].lower(), df['kw'].iloc[i].lower(), tokenizer, model)\n",
    "    except:\n",
    "        pass\n",
    "    else:\n",
    "        eco_embeddings_de.append(embedding)\n",
    "eco_embeddings_fr = []\n",
    "df = pd.read_csv('./domain_sentences/eco_sentences_fr.csv')\n",
    "for i in tqdm(range(len(df))):\n",
    "    try:\n",
    "        embedding = encoding_context(df['sentence'].iloc[i].lower(), df['kw'].iloc[i].lower(), tokenizer, model)\n",
    "    except:\n",
    "        pass\n",
    "        # print(df['sentence'].iloc[i], [df['kw'].iloc[i]])\n",
    "    else:\n",
    "        eco_embeddings_fr.append(embedding)\n",
    "tech_embeddings = []\n",
    "for kw, sentences  in tqdm(json.load(open('domain_sentences/c4istar_technologies_sentences.txt')).items()):\n",
    "    for sentence in sentences:\n",
    "        try:\n",
    "            tech_embeddings.append(encoding_context(sentence.lower(), kw.lower(), tokenizer, model))\n",
    "        except:\n",
    "            pass\n",
    "            # print(kw, sentence)\n",
    "eco_embeddings = []\n",
    "for kw, sentences  in tqdm(json.load(open('domain_sentences/eco_sentences.json')).items()):\n",
    "    for sentence in sentences:\n",
    "        try:\n",
    "            eco_embeddings.append(encoding_context(sentence.lower(), kw.lower(), tokenizer, model))\n",
    "        except Exception as e:\n",
    "            pass\n",
    "            # print(e, kw, sentence)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69313d90-b74e-42a6-97c2-7c86d6ae58e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(tech_embeddings)\n",
    "random.shuffle(tech_embeddings_de)\n",
    "random.shuffle(tech_embeddings_fr)\n",
    "random.shuffle(eco_embeddings)\n",
    "random.shuffle(eco_embeddings_de)\n",
    "random.shuffle(eco_embeddings_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5367e932-ed56-4889-bf6b-21e6c2f05076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7971811619113097"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC(gamma='auto',probability=True)\n",
    "clf.fit(tech_embeddings+eco_embeddings[:len(tech_embeddings)], [0]*len(tech_embeddings)+[1]*len(tech_embeddings))\n",
    "clf.score(tech_embeddings_fr+tech_embeddings_de+eco_embeddings_fr+eco_embeddings_de, [0]*len(tech_embeddings_fr+tech_embeddings_de)+[1]*len(eco_embeddings_fr+eco_embeddings_de))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e00be76c-7f31-4924-babe-12d06fb69b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(clf, open('svm.pkl','wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60820baa-8623-4704-9ce6-4b81607f51a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
